# ğŸ¤– Intelligent Navigation RL Research  

"To address the challenges of **low learning efficiency and poor generalization** in reinforcement learning for visual navigation tasks, we focus on image-goal navigation in the Habitat simulator and explore a technical pathway from model-free to model-based reinforcement learning approaches. Compared with existing methods on the image-goal navigation benchmark, our approach demonstrates significant performance improvements across three standard benchmark datasets (Gibson, MP3D, and HM3D)."

---  

## ğŸš€ 1. Model-free

ğŸ™Œ Official implementation of IEEE Robotics and Automation Letters accepted paper ["A New Representation of Universal Successor Features for Enhancing the Generalization of Target-driven Visual Navigation"](https://ieeexplore.ieee.org/document/10623277)

---  
#### ğŸ” 1.1 Research Background 
- **Problem Definition**: How to enhance agent generalization in target-driven visual navigation tasks?  
- **Research Significance**: Traditional methods perform poorly in new targets or environments, lacking universality.  
- **Challenges**: Complex state space and diverse targets  

#### ğŸ›°ï¸ 1.2 Research Methods  
- The framework incorporates Successor Features into the A3C architecture.ï¼ˆDerived from cognitive science principles, SF emulates neural mechanisms for constructing reusable predictive maps. This approach achieves reward-dynamics decomposition, facilitating rapid policy adaptation to reward modifications and enabling the acquisition of transferable environmental dynamics representations across task distributions.ï¼‰
- <details> <summary>ğŸ“ ä¸­æ–‡ç¿»è¯‘</summary>å°†SFä¸A3Cç®—æ³•ç»“åˆã€‚SFæºè‡ªè®¤çŸ¥ç§‘å­¦é¢†åŸŸï¼Œæ¨¡æ‹Ÿå¤§è„‘å¦‚ä½•åˆ›å»ºå¯é‡ç”¨çš„é¢„æµ‹åœ°å›¾ã€‚å°†å¥–åŠ±å’Œç¯å¢ƒåŠ¨æ€è§£è€¦ï¼Œä½¿å¾—ç­–ç•¥å¯ä»¥å¿«é€Ÿé€‚åº”å¥–åŠ±å˜åŒ–ï¼Œèƒ½å¤Ÿå­¦ä¹ å¤šä¸ªä»»åŠ¡ä¹‹é—´å¯è¿ç§»çš„ç¯å¢ƒåŠ¨æ€è¡¨å¾ã€‚</details>
- Implementation of state-feature-based prediction mechanisms to establish parsimonious dynamics models in latent space for SF estimation.
- <details> <summary>ğŸ“ ä¸­æ–‡ç¿»è¯‘</summary>ä½¿ç”¨çŠ¶æ€ç‰¹å¾é¢„æµ‹SFæ¥åˆ›å»ºæ½œåœ¨çš„ç®€çº¦åŠ¨åŠ›å­¦æ¨¡å‹ã€‚</details>
- Acquisition of compact rule sets within the latent state manifold to optimize successor feature prediction and extraction, enhancing the model's representational capacity.<details> <summary>ğŸ“ ä¸­æ–‡ç¿»è¯‘</summary>åœ¨æ½œåœ¨çŠ¶æ€ä¸­å­¦ä¹ è§„åˆ™é›†ï¼Œæœ‰åŠ©äºé¢„æµ‹å’Œè·å–åç»§ç‰¹å¾ã€‚</details>

![Example Image](Train/figs/SF.jpg)  
  
#### ğŸ† 1.3 Experimental Results  
- **Datasets**: Tested in multiple simulation environments (e.g., AI2-THOR, Habitat)  
- **Performance Metrics**: Success Rate (SR), Success weighted by Path Length (SPL)  
- **Conclusions**:  
  - Proposed USF method outperforms existing approaches in new targets and environments  
  - Significant improvement in generalization capability  



## ğŸš€ 2. Model-free
---  
ğŸ™Œ Official implementation of IROS 2025 under-review paper "Towards Efficient Image-Goal Navigation: A Self-Supervised Transformer-Based Reinforcement Learning Approach"

#### ğŸ” 2.1 Research Background 
- **Problem Definition**: How to improve efficiency and robustness in image-goal navigation tasks?  
- **Research Significance**: Traditional methods rely on extensive labeled data, high training costs, limited generalization  
- **Challenges**:  
  - Efficient visual feature extraction needed for image-goal navigation  
  - Data scarcity limiting model performance  

#### ğŸ›°ï¸ 2.2 Research Methods  
- Proposed a self-supervised Transformer-based reinforcement learning framework  
- **Core Innovations**:  
  - Self-supervised learning for pre-training visual feature extraction module  
  - Designed Transformer-based policy network enhancing long-sequence dependency modeling  
- **Algorithm Flow**:  
  1. Self-supervised Pre-training: Learning visual features through masked prediction tasks  
  2. Reinforcement Learning: Optimizing policy using PPO algorithm based on pre-training  

#### ğŸ† 2.3 Experimental Results  
- **Datasets**: Experiments conducted in Gibson and Matterport3D environments  
- **Performance Metrics**: Success Rate (SR), SPL, Training Time  
- **Conclusions**:  
  - Self-supervised pre-training significantly reduced training time  
  - Proposed Transformer policy network excels in complex environments  

![Example Image](Train/figs/Masked.jpg)  

## ğŸš€ 3. Model-based
---  

ğŸ™Œ Official implementation of CoRL 2025 under-preparation paper "Learning Stochastic World Models with VAE-Transformer for Visual Navigation (In Progress)"

#### ğŸ” 3.1 Research Background  
- **Problem Definition**: How to model environmental uncertainty in visual navigation?  
- **Research Significance**: Current world models lack stochastic modeling capabilities  
- **Challenges**:  
  - Complex environmental dynamics  
  - Uncertainty quantification in navigation  

#### ğŸ›°ï¸ 3.2 Research Methods   
- Developing a VAE-Transformer hybrid architecture for world modeling  
- **Core Components**:  
  - VAE for stochastic state representation  
  - Transformer for temporal dependency modeling  
- **Key Features**:  
  - Probabilistic state transitions  
  - Uncertainty-aware planning  

#### ğŸ† 3.3 Experimental Results  
- **Metrics**:  
  - Navigation success rate under uncertainty  
  - Model prediction accuracy  
- **Potential Impact**:  
  - More robust navigation in uncertain environments  
  - Better generalization to real-world scenarios
![Example Image](Train/figs/Network.jpg)  

ğŸ—‚ï¸ [Research in progress, results pending]
