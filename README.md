# ğŸ¤– Intelligent Navigation RL Research  
![Ubuntu](https://img.shields.io/badge/OS-Ubuntu%2020.04-orange)
[![MIT License](https://img.shields.io/badge/License-MIT-green.svg)](https://choosealicense.com/licenses/mit/)
[![Python](https://img.shields.io/badge/Python-3.8%2B-blue)](https://www.python.org/)
[![Habitat Lab](https://img.shields.io/badge/Habitat-Lab-blue)](https://aihabitat.org/docs/habitat-lab/)
[![Contributions Welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat)](https://github.com/hujch23/World-Model-Navigation/issues)
## ğŸ“š Content

- [0. Overview](#ğŸŒŸ0.Overview)
- [1. Model-free-I-Image-goal Navigation](#ğŸ”¬1.Model-free-I)
- [2. Model-free-II-Image-goal Navigation](#2.Model-free-II)
- [3. Model-based-I-Image-goal Navigation](#3.Model-based-IVisuaLanguagelNavigation)
- [4. Model-based-II-Visual Language Navigation](#4.Mode-l-based-IIVisualLanguageNavigation)
- [5. Install and Prepare Datasets for Image-goal Navigation](#5.InstallandPreparedataset)


## ğŸŒŸ 0. Overview

To address the challenges of **low learning efficiency, poor generalization, and sim-to-real transfer** in reinforcement learning for visual navigation tasks, we focus on image-goal navigation in the Habitat simulator and explore a technical pathway from model-free to model-based reinforcement learning approaches. Compared with existing methods on the image-goal navigation benchmark, our approach demonstrates significant performance improvements across three standard benchmark datasets (Gibson, MP3D, and HM3D).

---  
![Example Image](video.gif) 


## ğŸ”¬ 1. Model-free-I

ğŸ™Œ Official implementation of IEEE Robotics and Automation Letters accepted paper ["A New Representation of Universal Successor Features for Enhancing the Generalization of Target-driven Visual Navigation"](https://ieeexplore.ieee.org/document/10623277)

---  
#### ğŸš€ 1.1 Research Background 
- **Problem Definition**: How to address poor generalization in reinforcement learning for visual navigation task.  
- **Research Significance**: Traditional methods perform poorly in new targets or environments, lacking universality.  
- **Challenges**: Complex state spaces and diverse goal representations.

#### ğŸ›°ï¸ 1.2 Research Methods  
- The framework incorporates Successor Features into the A3C architecture.ï¼ˆDerived from cognitive science principles, SF emulates neural mechanisms for constructing reusable predictive maps. This approach achieves reward-dynamics decomposition, facilitating rapid policy adaptation to reward modifications and enabling the acquisition of transferable environmental dynamics representations across task distributions.ï¼‰ğŸ“ ä¸­æ–‡ç¿»è¯‘ï¼šå°†SFä¸A3Cç®—æ³•ç»“åˆã€‚SFæºè‡ªè®¤çŸ¥ç§‘å­¦é¢†åŸŸï¼Œæ¨¡æ‹Ÿå¤§è„‘å¦‚ä½•åˆ›å»ºå¯é‡ç”¨çš„é¢„æµ‹åœ°å›¾ã€‚å°†å¥–åŠ±å’Œç¯å¢ƒåŠ¨æ€è§£è€¦ï¼Œä½¿å¾—ç­–ç•¥å¯ä»¥å¿«é€Ÿé€‚åº”å¥–åŠ±å˜åŒ–ï¼Œèƒ½å¤Ÿå­¦ä¹ å¤šä¸ªä»»åŠ¡ä¹‹é—´å¯è¿ç§»çš„ç¯å¢ƒåŠ¨æ€è¡¨å¾ã€‚
- Implementation of state-feature-based prediction mechanisms to establish parsimonious dynamics models in latent space for SF estimation. ğŸ“ ä¸­æ–‡ç¿»è¯‘ï¼šä½¿ç”¨çŠ¶æ€ç‰¹å¾é¢„æµ‹SFæ¥åˆ›å»ºæ½œåœ¨çš„ç®€çº¦åŠ¨åŠ›å­¦æ¨¡å‹ã€‚
- Acquisition of compact rule sets within the latent state manifold to optimize successor feature prediction and extraction, enhancing the model's representational capacity.ğŸ“ ä¸­æ–‡ç¿»è¯‘ï¼šåœ¨æ½œåœ¨çŠ¶æ€ä¸­å­¦ä¹ è§„åˆ™é›†ï¼Œæœ‰åŠ©äºé¢„æµ‹å’Œè·å–åç»§ç‰¹å¾ã€‚

![Example Image](figs/SF.jpg)  
  
#### ğŸ† 1.3 Experimental Results  
- **Datasets**: Tested in multiple simulation environments (e.g., AI2-THOR, Habitat). 
- **Performance Metrics**:
  - Success Rate (SR)
  - Success weighted by Path Length (SPL)
  - Continuous Learning Performance 
- **Conclusions**:
  - Achieving state-of-the-art performance in the generalization of target, scenarios, and domains.
  - Demonstrating strong resistance to catastrophic forgetting in continual learning.



## ğŸ”® 2. Model-free-II
---  
ğŸ™Œ Official implementation of IROS 2025 under-review paper "Towards Efficient Image-Goal Navigation: A Self-Supervised Transformer-Based Reinforcement Learning Approach".

#### ğŸš€ 2.1 Research Background 
- **Problem Definition**: How to improve the cross-scene and cross-domain generalization ability in visual navigation, enabling agents to effectively navigate to target locations in new environments.  
- **Research Significance**: Existing methods face challenges in handling long-term temporal information and cross-domain generalization. There is a need for more effective visual representation learning and temporal information processing methods.  
- **Challenges**:
  - Simultaneous processing of high-dimensional visual inputs and complex temporal dependencies.
  - Challenges from dynamic camera parameter settings in real-world scenarios.

#### ğŸ›°ï¸ 2.2 Research Methods  
- Designed a Transformer-based dual attention mechanism framework.
- Bidirectional attention for masked prediction learning to enhance representation capability.
-  Causal attention for generating belief states to guide policy decisions.
-  Shared Transformer network to reduce parameter count.

![Example Image](figs/Masked.jpg)  

#### ğŸ† 2.3 Experimental Results  
- **Datasets**:
  - Gibson dataset (training and testing).
  - MP3D and HM3D datasets (cross-domain testing).
  - Categorized by difficulty: easy (1.5-3m), medium (3-5m), and hard (5-10m). 
- **Performance Metrics**:
  - Success Rate (SR)
  - Success weighted by Path Length (SPL)
- **Conclusions**:  
  - Achieved state-of-the-art performance on Gibson dataset, demonstrating superior navigation capabilities (SR 91.7%, SPL 68.5%).
  - Exhibited robust cross-domain generalization, maintaining consistent performance across diverse environments (MP3D: SR 79.1%, SPL 52.8%; HM3D: SR 79.1%, SPL 46.6%).
  - Demonstrated remarkable resilience to dynamic camera parameters (height: 0.8m-1.5m, pitch: Â±5Â°, FoV: 60Â°-120Â°) (Gibson SR 74.1%, SPL 48.9%).
  - Demonstrated successful real-world deployment on a mobile robot equipped with NVIDIA Jetson Orin NX, achieving reliable navigation performance in cluttered office environments.
    

## ğŸ¯ 3. Model-basedï¼ˆWorld Modelï¼‰
---  

ğŸ™Œ Official implementation of CoRL 2025 under-preparation paper "Learning Stochastic World Models with CVAE-Transformer for Visual Navigation (In Progress)"

#### ğŸš€ 3.1 Research Background  
- **Problem Definition**: Model-based visual navigation methods face challenges in modeling environmental uncertainty and accumulating prediction errors during long-horizon planning. Current approaches using deterministic world models often fail to capture the stochastic nature of real-world environments
- **Research Significance**:
  - Sample Efficiency: Model-based methods enable self-supervised learning of environment dynamics, significantly improving sample efficiency compared to model-free approaches
  - Uncertainty Modeling: Existing methods lack robust uncertainty quantification, leading to potential navigation failures
  - Training Efficiency: Current world models require lengthy training periods with limited performance gains
- **Challenges**:  
  - Prediction Error Accumulationï¼šAuto-regressive "imagination" processes lead to compounding errorsã€Trajectory deviations cause agents to pursue virtual targets  
  - Environmental Complexityï¼šDynamic and uncertain real-world environmentsã€Complex state transitions and action effects

#### ğŸ›°ï¸ 3.2 Research Methods   
- We propose a stochastic Transformer-based world model that combines a Categorical VAE for robust state encoding with a GPT-style causal architecture for efficient sequence modeling
- The CVAE encoder improves agent robustness and reduces cumulative errors in auto-regressive prediction by capturing environmental uncertainties in a structured latent space
- The causal Transformer with identifiable factorization enhances both modeling quality and generation capabilities while accelerating the training process through efficient temporal dependency learning  

![Example Image](figs/Network.jpg) 

#### ğŸ† 3.3 Experimental Results  

ğŸ—‚ï¸ [Research in progress, results pending]

## ğŸ§¸ 4. Visual language navigation world model
ğŸ—‚ï¸ [Research in progress, results pending]

## ğŸ‚ 5. Install and Prepare Datasets for Image-goal Navigation

### ğŸ‘’ 5.1 Install habitat-lab 
```bash
# clone our repo
git clone https://github.com/hujch23/World-Model-Navigation.git
cd World-Model-Navigation

# clone habitat-lab code
git submodule init
git submodule update

# create conda env
conda create -n World-Model-Navigation python=3.8

# install habitat-sim
conda install habitat-sim=0.2.2 withbullet headless -c conda-forge -c aihabitat

# install pytorch1.11
pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 -f https://download.pytorch.org/whl/torch_stable.html

# install habitat-lab and habitat-baselines
cd habitat-lab
git checkout 1f7cfbdd3debc825f1f2fd4b9e1a8d6d4bc9bfc7
pip install -e habitat-lab 
pip install -e habitat-baselines

pip install -r requirements.txt
```

### ğŸ—‚ï¸ 5.2  Download Scene Datasets
You can download the datasets from https://github.com/XinyuSun/FGPrompt. As mentioned in the repository, follow the official guidelines to download Gibson, HM3D, and MP3D scene datasets and place them in the data/scene_datasets directory.

### ğŸ“‘ 5.3  Prepare Train and Test episodes
We provide the script `datasets.py` to ensure the smooth loading of both curved and straight test episodes.



